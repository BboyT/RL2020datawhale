{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习讨论的问题是一个agent在一个复杂不确定的环境里面去极大化它能获得Ω的奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delay reward\n",
    "exoloration\n",
    "exploitation\n",
    "reward signal \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习的特征：\n",
    "* trail-and-error exploration \n",
    "* agent 从环境里面获得延迟的奖励\n",
    "* 时间非常重要\n",
    "* agent的行为会影响它随后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，它需要探索患者的病情，来获取对患者疾病状态的理解（trail-and-error exploration）\n",
    "第二，agent会从患者的生命体征、生理病理状态获得延迟的奖励\n",
    "第三，患者需要时间接受治疗\n",
    "第四，每个治疗方案会影响它随后的数据\n",
    "\n",
    "其中奖励即为：患者的生存时间7天是否死亡、28天是否死亡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态\n",
    "环境\n",
    "观测\n",
    "离散动作空间\n",
    "连续动作空间\n",
    "agent会用一个policy function\n",
    "policy： stochastic polivy；deterministic policy\n",
    "可能会生成一个value function\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actor-critic agent\n",
    "基于价值迭代：Q-learning\n",
    "基于策略迭代：策略梯度算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体执行动作前，是否能对下一步的状态和奖励进行预测，如果可以，就能够对环境进行建模，从而采用有模型学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前免模型学习占主流，更为简单直观且有丰富的开源资料\n",
    "大部分情况下环境都是静态、可描述的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习的基本结构是什么？\n",
    "\n",
    "Agent和Environment间的交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习相对于监督学习为什么训练会更加困难？\n",
    "\n",
    "1、数据为序列数据\n",
    "2、无标签、延迟奖励\n",
    "3、时间\n",
    "4、agent的action影响随后得到的反馈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近几年强化学习发展迅速的原因？\n",
    "\n",
    "1、算力的提示\n",
    "2、有了深度强化学习这样end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态和观测有什么关系？\n",
    "\n",
    "状态（state）是对世界的**完整描述**，不会隐藏世界的信息。观测（observation）是对状态的**部分描述**，可能会遗漏一些信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个强化学习 Agent，它由什么组成？\n",
    "\n",
    "\n",
    "\n",
    "1. **策略函数（policy function）**，Agent会用这个函数来选取它下一步的动作，包括**随机性策略（stochastic policy）**和**确定性策略（deterministic policy）**。\n",
    "\n",
    "2. **价值函数（value function）**，我们用价值函数来对当前状态进行评估，即进入现在的状态，到底可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。\n",
    "\n",
    "3. **模型（model）**，其表示了 Agent 对这个Environment的状态进行的理解，它决定了这个系统是如何进行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据强化学习 Agent 的不同，我们可以将其分为哪几类？\n",
    "\n",
    "actor-critic agent\n",
    "基于价值迭代：Q-learning\n",
    "基于策略迭代：策略梯度算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于策略迭代和基于价值迭代的强化学习方法有什么区别?\n",
    "\n",
    "\n",
    "\n",
    "1. 基于策略迭代的强化学习方法，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。\n",
    "2. 基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)；\n",
    "3. 基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。\n",
    "4. 此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有模型（model-based）学习和免模型（model-free）学习有什么区别？\n",
    "\n",
    "根据是否需要对真实环境建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习的通俗理解\n",
    "\n",
    "\n",
    "首先，它需要探索患者的病情，来获取对患者疾病状态的理解（trail-and-error exploration）\n",
    "第二，agent会从患者的生命体征、生理病理状态获得延迟的奖励\n",
    "第三，患者需要时间接受治疗\n",
    "第四，每个治疗方案会影响它随后的数据\n",
    "\n",
    "其中奖励可为：患者的生存时间7天是否死亡、28天是否死亡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
