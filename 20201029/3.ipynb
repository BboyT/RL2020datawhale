{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi$\n",
    "\n",
    "$\\theta$ 来代表 $\\pi$ 的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一场游戏叫做一个 `episode(回合)` 或者 `trial(试验)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有得到的 reward 都总合起来，就是 `total reward`，我们称其为`return(回报)`，用 R 来表示它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text { Trajectory } \\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t}\\right\\} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 如何maximize expected reward 呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient ascent 在 update 参数的时候要加。要进行 gradient ascent，我们先要计算 expected reward $\\bar{R}$ 的 gradient 。我们对 $\\bar{R}$ 取一个 gradient，这里面只有 $p_{\\theta}(\\tau)$ 是跟 $\\theta$ 有关，所以 gradient 就放在 $p_{\\theta}(\\tau)$ 这个地方。$R(\\tau)$ 这个 reward function 不需要是 differentiable，我们也可以解接下来的问题。举例来说，如果是在 GAN 里面，$R(\\tau)$ 其实是一个 discriminator，它就算是没有办法微分，也无所谓，你还是可以做接下来的运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以对 $\\nabla p_{\\theta}(\\tau)$ 使用这个公式，然后会得到 $\\nabla p_{\\theta}(\\tau)=p_{\\theta}(\\tau)  \\nabla \\log p_{\\theta}(\\tau)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意 $p_{\\theta}(\\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自于 environment，$p_\\theta(a_t|s_t)$ 是来自于 agent。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现的时候，把state类比为分类器中的input，output类比为每个行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现的Tips："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.add a baseline:reward总是正数，可基于Reward的期望来设定一个weight。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Assign Suitable Credit:每个action不同的weight，可反映每一个action到底是好还是不好。每个action之后产生的reward累计再减去baseline，才了解这个行为真实所做的贡献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC：蒙特卡洛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据learn的agent和环境互动的agent是否为同一个区分为：\n",
    "\n",
    "\n",
    "1.on-policy\n",
    "\n",
    "2.off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决sample data不能重复使用，为了节约时间，所以想要off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把 $x^i$ 代到 $f(x)$ 里面，然后取它的平均值，就可以近似 $f(x)$ 的期望值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample需要够多次，不然方差会大很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\int f(x) p(x) d x=\\int f(x) \\frac{p(x)}{q(x)} q(x) d x=E_{x \\sim q}[f(x){\\frac{p(x)}{q(x)}}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变换：on-policy training 的 algorithm 改成 off-policy training 的 algorithm。\n",
    "\n",
    "之前我们是拿 $\\theta$ 这个 policy 去跟环境做互动，sample 出 trajectory $\\tau$，然后计算 $R(\\tau) \\nabla \\log p_{\\theta}(\\tau)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们不用 $\\theta$ 去跟环境做互动，假设有另外一个 policy  $\\theta'$，它就是另外一个actor。它的工作是他要去做demonstration，$\\theta'$ 的工作是要去示范给$\\theta$ 看。它去跟环境做互动，告诉 $\\theta$ 说，它跟环境做互动会发生什么事。然后，借此来训练$\\theta$。我们要训练的是 $\\theta$ ，$\\theta'$  只是负责做 demo，负责跟环境做互动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在的 $\\tau$ 是从 $\\theta'$ sample 出来的，是拿 $\\theta'$ 去跟环境做互动。所以 sample 出来的 $\\tau$ 是从 $\\theta'$ sample 出来的，这两个distribution 不一样。但没有关系，假设你本来是从 p 做 sample，但你发现你不能够从 p 做sample，所以我们不拿$\\theta$ 去跟环境做互动。你可以把 p 换 q，然后在后面这边补上一个 importance weight。现在的状况就是一样，把 $\\theta$ 换成 $\\theta'$ 后，要补上一个 importance weight $\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\\prime}}(\\tau)}$。这个 importance weight 就是某一个 trajectory $\\tau$ 用 $\\theta$ 算出来的概率除以这个 trajectory $\\tau$，用 $\\theta'$ 算出来的概率。这一项是很重要的，因为今天你要 learn 的是 actor $\\theta$ 和 $\\theta'$ 是不太一样的。$\\theta'$ 会见到的情形跟 $\\theta$ 见到的情形不见得是一样的，所以中间要做一个修正的项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 model 是 $\\theta$ 的时候，你看到$s_t$ 的概率，跟 model 是$\\theta'$  的时候，你看到$s_t$ 的概率是差不多的，即$p_{\\theta}(s_t)=p_{\\theta'}(s_t)$。因为它们是一样的，所以你可以把它删掉。\n",
    "\n",
    "$p_{\\theta}(a_t|s_t)$很好算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
